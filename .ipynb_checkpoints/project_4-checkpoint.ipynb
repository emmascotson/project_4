{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c185b3d-1a9c-4f51-b364-e49fcb48ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/emmascotson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emmascotson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/emmascotson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/emmascotson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/emmascotson/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import plotly.express as px\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import classification_report, silhouette_score, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from scipy.stats import chi2_contingency, multinomial\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer, regexp_tokenize, sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import SnowballStemmer, LancasterStemmer, PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import WordNetLemmatizer, pos_tag\n",
    "from nltk.util import bigrams\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "#nltk.download('all')\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "import itertools\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from joblib import parallel_backend\n",
    "\n",
    "from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537920c8-8909-4079-9ca8-8034eb10675e",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "Let's load our dataset and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f51c85ff-34c5-4052-bf1b-f21e9378eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Combined Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa4ea2d9-5143-4f91-9694-d4d894d48cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I've shifted my focus to something else but I'...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I'm restless and restless, it's been a month n...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          statement   status\n",
       "0           0                                         oh my gosh  Anxiety\n",
       "1           1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
       "2           2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
       "3           3  I've shifted my focus to something else but I'...  Anxiety\n",
       "4           4  I'm restless and restless, it's been a month n...  Anxiety"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18f3dfbb-04a1-47ca-8518-fc02769e29cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53043 entries, 0 to 53042\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  53043 non-null  int64 \n",
      " 1   statement   52681 non-null  object\n",
      " 2   status      53043 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154afd32-9475-4d8e-9340-0b61f42f46ab",
   "metadata": {},
   "source": [
    "Looks like we have some NaNs. Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "375f4934-3cfd-4e12-968a-c2beee9290b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>1539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>2448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52838</th>\n",
       "      <td>52838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52870</th>\n",
       "      <td>52870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52936</th>\n",
       "      <td>52936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53010</th>\n",
       "      <td>53010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53031</th>\n",
       "      <td>53031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 statement   status\n",
       "293           293       NaN  Anxiety\n",
       "572           572       NaN  Anxiety\n",
       "595           595       NaN  Anxiety\n",
       "1539         1539       NaN   Normal\n",
       "2448         2448       NaN   Normal\n",
       "...           ...       ...      ...\n",
       "52838       52838       NaN  Anxiety\n",
       "52870       52870       NaN  Anxiety\n",
       "52936       52936       NaN  Anxiety\n",
       "53010       53010       NaN  Anxiety\n",
       "53031       53031       NaN  Anxiety\n",
       "\n",
       "[362 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[pd.isna(data['statement'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b7941-4506-4eaa-a9f2-54f63ae71830",
   "metadata": {},
   "source": [
    "There are so few NaN's compared to the number of rows in our entire dataframe. Furthermore, rows with no 'statement' value are useless to us. Let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd2e6f9f-a4bd-414c-a240-51743dee6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['statement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb74db-ea35-4023-bd9e-6c5e92cc541c",
   "metadata": {},
   "source": [
    "Now let's check if there are duplicate rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f691929-f234-46e6-b10b-31cb0c12dcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51073"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['statement'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26078e20-7f40-407a-891e-fb1d8f2f093d",
   "metadata": {},
   "source": [
    "It looks like there are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f729d8cc-a2fe-4a48-8d5b-141351d5f285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>\"No regrets or grudges/angry at things that ha...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>138</td>\n",
       "      <td>but my heart is still restless even though my ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>167</td>\n",
       "      <td>I want to exhale the restlessness in my chest ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>228</td>\n",
       "      <td>Do not compare yourself to others. Envy only m...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>244</td>\n",
       "      <td>people seem calm, happy like there's no proble...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53038</th>\n",
       "      <td>53038</td>\n",
       "      <td>Nobody takes me seriously I’ve (24M) dealt wit...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53039</th>\n",
       "      <td>53039</td>\n",
       "      <td>selfishness  \"I don't feel very good, it's lik...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53040</th>\n",
       "      <td>53040</td>\n",
       "      <td>Is there any way to sleep better? I can't slee...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53041</th>\n",
       "      <td>53041</td>\n",
       "      <td>Public speaking tips? Hi, all. I have to give ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53042</th>\n",
       "      <td>53042</td>\n",
       "      <td>I have really bad door anxiety! It's not about...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1608 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                          statement   status\n",
       "97             97  \"No regrets or grudges/angry at things that ha...  Anxiety\n",
       "138           138  but my heart is still restless even though my ...  Anxiety\n",
       "167           167  I want to exhale the restlessness in my chest ...  Anxiety\n",
       "228           228  Do not compare yourself to others. Envy only m...  Anxiety\n",
       "244           244  people seem calm, happy like there's no proble...  Anxiety\n",
       "...           ...                                                ...      ...\n",
       "53038       53038  Nobody takes me seriously I’ve (24M) dealt wit...  Anxiety\n",
       "53039       53039  selfishness  \"I don't feel very good, it's lik...  Anxiety\n",
       "53040       53040  Is there any way to sleep better? I can't slee...  Anxiety\n",
       "53041       53041  Public speaking tips? Hi, all. I have to give ...  Anxiety\n",
       "53042       53042  I have really bad door anxiety! It's not about...  Anxiety\n",
       "\n",
       "[1608 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['statement'].duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e508465-bd55-4811-ae62-605cec7aa8f2",
   "metadata": {},
   "source": [
    "These statements all look pretty specific and personalized. AKA it does seem like our duplicate values are true duplicates of the same social media statements made by the same singular user. Furthermore, we have over 50,000 rows in our dataset and can afford to lose these rows. \n",
    "\n",
    "Let's drop duplicates as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb56cb74-4133-41f3-bd6d-dbbf3070b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'statement' duplicates, keeping first instance\n",
    "data = data.drop_duplicates(subset=['statement'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f702d642-da17-491a-840a-014e58d72aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, statement, status]\n",
       "Index: []"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['statement'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe3b0350-ba43-41ed-9a7c-35abfdf9b410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 51073 entries, 0 to 52840\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  51073 non-null  int64 \n",
      " 1   statement   51073 non-null  object\n",
      " 2   status      51073 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345cd56-4f66-46b9-9b1d-d808ddab995c",
   "metadata": {},
   "source": [
    "Furthermore, our 'Unnamed: 0' column appears to just be a duplicate of our index. Let's explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93329d1b-a62b-4866-9f1c-fff9c84d6ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51073"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Unnamed: 0'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "949e863d-6ba4-4e99-8000-ddf9fe628dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if 'Unnamed: 0' equals our index\n",
    "is_identical = data['Unnamed: 0'].equals(data.index)\n",
    "is_identical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a471566-b569-4597-a1b1-78c6eef57fe1",
   "metadata": {},
   "source": [
    "Hm...We can safely assume this column doesn't provide information about multiple 'statement' values being generated by the same singular user...because our nunique() matches the total number of rows in our dataframe, which eliminates the possibility of duplicates. \n",
    "\n",
    "This column is most likely just an old index from previous datasets, which is no longer useful to use. We'll drop it for now. We can always go back and edit the code if we decide we need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c08e530c-fbd3-4a43-910b-6ee75bedd5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412fd6db-5718-418b-b1e5-eb4e8b4fe12c",
   "metadata": {},
   "source": [
    "Let's look at the value_counts() for our target variable 'status'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20ba884d-fbe9-4492-8845-3c304447b8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Normal                  16039\n",
       "Depression              15087\n",
       "Suicidal                10641\n",
       "Anxiety                  3617\n",
       "Bipolar                  2501\n",
       "Stress                   2293\n",
       "Personality disorder      895\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49bd72-d483-4b66-a108-05eba079b805",
   "metadata": {},
   "source": [
    "#### 'Personality disorder'\n",
    "\n",
    "'Personality disorder' is an ambiguous label compared to the others. It might be a version of a 'placeholder' value for not 'Normal' users where further categorization of disorder (ex. 'Depression', 'Suicidal', 'Anxiety') was inconclusive. Alternatively, it could be indication of other, entirely different personality disorders not listed above. \n",
    "\n",
    "We'll keep this in mind and explore as we go. Depending on what we find, it might be helpful to drop these values when building a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbcea0-3ffe-4d21-acaf-6b2eff2148f5",
   "metadata": {},
   "source": [
    "# Preprocessing & EDA\n",
    "\n",
    "## Prior to Cleaning...\n",
    "\n",
    "We'll want to clean the text in our data by performing operations that remove punctuation and special characters, lowercase text, remove newline '\\n' characters, etc.\n",
    "\n",
    "However, ***before*** we do this...let's look at whether or not we can use any of our 'pre-cleaned' text to create features that might be useful down the road.\n",
    "\n",
    "#### Percentage Upper-Case\n",
    "\n",
    "Let's think about whether or not any unique punctuation and/or text characteristics might be indicative of a personality disorder. \n",
    "\n",
    "*Upper-case* text might help identify whether or not a person is in some kind of distress. If user posts 'A MESSAGE ENTIRELY IN UPPER-CASE LIKE SO', that's an unusual behavior that we should try and quantify.\n",
    "\n",
    "Let's create a new column 'perc_upper' that calculates the percentage of upper-case letters to the total number of letters in each 'statement'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b57c3989-1507-4906-b743-f03cd7430821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate percentage of upper-case letters\n",
    "def perc_upper(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    letters = re.findall(r'[a-zA-Z]', text) # Using regex to isolate letters\n",
    "    if not letters: \n",
    "        return 0\n",
    "    upper_count = len(re.findall(r'[A-Z]', text))\n",
    "    return (upper_count / len(letters)) * 100\n",
    "\n",
    "data['perc_upper'] = data['statement'].apply(perc_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8fc74ec-d805-4cec-9206-f6184e86c1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "      <th>perc_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3546</th>\n",
       "      <td>[HELP RT] WE FANBASE SHAKE RP! JOIN? FOLLOW FI...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>ONAKA GA SUITA</td>\n",
       "      <td>Normal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6787</th>\n",
       "      <td>TODAY NO CLASS YAAYYY</td>\n",
       "      <td>Normal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>6 HALF HOURS AGAINIII ULULU I ​​WANT TO SLEEP ...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>I DON'T HAVE A HOLIDAY AS WELL AS EVIL</td>\n",
       "      <td>Normal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              statement  status  perc_upper\n",
       "3546  [HELP RT] WE FANBASE SHAKE RP! JOIN? FOLLOW FI...  Normal       100.0\n",
       "2436                                     ONAKA GA SUITA  Normal       100.0\n",
       "6787                              TODAY NO CLASS YAAYYY  Normal       100.0\n",
       "5303  6 HALF HOURS AGAINIII ULULU I ​​WANT TO SLEEP ...  Normal       100.0\n",
       "2478             I DON'T HAVE A HOLIDAY AS WELL AS EVIL  Normal       100.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_sorted = data.sort_values(by='perc_upper', ascending=False)\n",
    "upper_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aca806-5998-46af-b4b0-cfbd3dfa41fa",
   "metadata": {},
   "source": [
    "#### METRIC CHANGE - Percentage by Upper-Case Words\n",
    "\n",
    "On second thought, we should find a different way to quantify a notable amount of upper-case in a string of text. With our current 'perc_upper' value...a statement such as 'Hi' will have a 50% perc_upper. However this would be due to normal grammatical capitalization techniques that are of no note.\n",
    "\n",
    "Let's alter our metric slightly, to calculate the **percentage of upper-case words compared to the total number of words in a statement**.\n",
    "\n",
    "We'll stick with regex for now to tokenize our words and calculate this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb0a1bc5-a2be-4951-b638-8db20b1fd054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping perc_upper column before to reduce computation time\n",
    "data.drop(columns=['perc_upper'], inplace=True)\n",
    "\n",
    "# Updating perc_upper to calculate based on number of words\n",
    "def perc_upper_words(text): \n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    words = re.findall(r'\\b[a-zA-Z\\'-]+\\b', text) # Including apostrophes and hyphens\n",
    "    words = [word for word in words if not re.search(r'\\d', word)] # Filter out numbers\n",
    "    if not words:\n",
    "        return 0\n",
    "    uppercase_words = [word for word in words if word.isupper()]\n",
    "    return len(uppercase_words) / len(words) * 100\n",
    "\n",
    "data['perc_upper_words'] = data['statement'].apply(perc_upper_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "de3912ef-534f-4851-9613-1325482ff6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "      <th>perc_upper_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11210</th>\n",
       "      <td>I KEEP MESSING THINGS UP ALL DAY EVERY DAY THE...</td>\n",
       "      <td>Suicidal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>#PECAT WHOSE NAME IS SI ALI MOCHTAR NYEBELIN #</td>\n",
       "      <td>Normal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>OH MY GOD FEAR CANCEL</td>\n",
       "      <td>Normal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>HIS BD CARD KANON IS CUTE HSHSHSHS SAD NO FREE...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>YAALLAH SO NATION OF SM DREAMIES</td>\n",
       "      <td>Normal</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               statement    status  \\\n",
       "11210  I KEEP MESSING THINGS UP ALL DAY EVERY DAY THE...  Suicidal   \n",
       "5917      #PECAT WHOSE NAME IS SI ALI MOCHTAR NYEBELIN #    Normal   \n",
       "4173                               OH MY GOD FEAR CANCEL    Normal   \n",
       "1581   HIS BD CARD KANON IS CUTE HSHSHSHS SAD NO FREE...    Normal   \n",
       "2811                    YAALLAH SO NATION OF SM DREAMIES    Normal   \n",
       "\n",
       "       perc_upper_words  \n",
       "11210             100.0  \n",
       "5917              100.0  \n",
       "4173              100.0  \n",
       "1581              100.0  \n",
       "2811              100.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_sorted = data.sort_values(by='perc_upper_words', ascending=False)\n",
    "upper_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73ad3bf6-a86d-4058-8142-2ff7f7792bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "      <th>perc_upper_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10953</th>\n",
       "      <td>NOBODY WANTS TO TALK TO ME Going to kill myself</td>\n",
       "      <td>Depression</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>the habit of chatting for a day, gadicchat imm...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17851</th>\n",
       "      <td>I feel like I cannot even explain myself becau...</td>\n",
       "      <td>Suicidal</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>NOT FUNNY MORNING â€ Runny nose</td>\n",
       "      <td>Normal</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24841</th>\n",
       "      <td>I try my best but I just want to die. I hate m...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>59.420290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6357</th>\n",
       "      <td>HAHAAAA GET THE ASSIGNMENT OF MAKING A SCIENTI...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>57.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>WANT TO SIN BUT Afraid to Laugh</td>\n",
       "      <td>Normal</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36232</th>\n",
       "      <td>Everything is temporary. EVIL EYES OFF SHIVRIT</td>\n",
       "      <td>Normal</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>MORNING AGAIN I've made a AMENDE SPACE</td>\n",
       "      <td>Normal</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48673</th>\n",
       "      <td>AAAAAAAAAAAAAAAAAA aaAAAAAAAAAA FIDBWJSKLA oaa...</td>\n",
       "      <td>Stress</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4557</th>\n",
       "      <td>if this passes, DROP YOUR LAST COPY !</td>\n",
       "      <td>Normal</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7490</th>\n",
       "      <td>This is what the government wants poor people ...</td>\n",
       "      <td>Suicidal</td>\n",
       "      <td>56.862745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19137</th>\n",
       "      <td>I am here to talk if you need help HAHAHAHAA A...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>56.756757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22163</th>\n",
       "      <td>I MIGHT KILL MSELF BECAUSE I am A RETARDED USE...</td>\n",
       "      <td>Suicidal</td>\n",
       "      <td>56.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23744</th>\n",
       "      <td>i just want to fit in. i do not want to be mad...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>56.074766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8319</th>\n",
       "      <td>Bitch why the fuck did you just tell me to che...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>56.020942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15768</th>\n",
       "      <td>Welp fuck it everything I do gets put on the b...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>55.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25969</th>\n",
       "      <td>My humiliation and mental torture of sleeping ...</td>\n",
       "      <td>Suicidal</td>\n",
       "      <td>55.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5121</th>\n",
       "      <td>type IMB space DENITAKOPLOK send to 14045, TON...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>55.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>I really regret talkin or sayin anything. I am...</td>\n",
       "      <td>Suicidal</td>\n",
       "      <td>54.255319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               statement      status  \\\n",
       "10953    NOBODY WANTS TO TALK TO ME Going to kill myself  Depression   \n",
       "498    the habit of chatting for a day, gadicchat imm...     Anxiety   \n",
       "17851  I feel like I cannot even explain myself becau...    Suicidal   \n",
       "4525                     NOT FUNNY MORNING â€ Runny nose      Normal   \n",
       "24841  I try my best but I just want to die. I hate m...  Depression   \n",
       "6357   HAHAAAA GET THE ASSIGNMENT OF MAKING A SCIENTI...      Normal   \n",
       "1113                     WANT TO SIN BUT Afraid to Laugh      Normal   \n",
       "36232     Everything is temporary. EVIL EYES OFF SHIVRIT      Normal   \n",
       "830               MORNING AGAIN I've made a AMENDE SPACE      Normal   \n",
       "48673  AAAAAAAAAAAAAAAAAA aaAAAAAAAAAA FIDBWJSKLA oaa...      Stress   \n",
       "4557               if this passes, DROP YOUR LAST COPY !      Normal   \n",
       "7490   This is what the government wants poor people ...    Suicidal   \n",
       "19137  I am here to talk if you need help HAHAHAHAA A...  Depression   \n",
       "22163  I MIGHT KILL MSELF BECAUSE I am A RETARDED USE...    Suicidal   \n",
       "23744  i just want to fit in. i do not want to be mad...  Depression   \n",
       "8319   Bitch why the fuck did you just tell me to che...  Depression   \n",
       "15768  Welp fuck it everything I do gets put on the b...  Depression   \n",
       "25969  My humiliation and mental torture of sleeping ...    Suicidal   \n",
       "5121   type IMB space DENITAKOPLOK send to 14045, TON...      Normal   \n",
       "8227   I really regret talkin or sayin anything. I am...    Suicidal   \n",
       "\n",
       "       perc_upper_words  \n",
       "10953         60.000000  \n",
       "498           60.000000  \n",
       "17851         60.000000  \n",
       "4525          60.000000  \n",
       "24841         59.420290  \n",
       "6357          57.894737  \n",
       "1113          57.142857  \n",
       "36232         57.142857  \n",
       "830           57.142857  \n",
       "48673         57.142857  \n",
       "4557          57.142857  \n",
       "7490          56.862745  \n",
       "19137         56.756757  \n",
       "22163         56.521739  \n",
       "23744         56.074766  \n",
       "8319          56.020942  \n",
       "15768         55.882353  \n",
       "25969         55.882353  \n",
       "5121          55.555556  \n",
       "8227          54.255319  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling rows from the middle\n",
    "filtered_rows = upper_sorted[(upper_sorted['perc_upper_words'] >= 50.0) & (upper_sorted['perc_upper_words'] <= 60.0)]\n",
    "filtered_rows.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c0284-4c55-4ca3-be98-a03528f616a0",
   "metadata": {},
   "source": [
    "Let's see if there are any trends regarding the distribution of these numbers within each of our 'status' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "057a6f96-febb-43c6-a668-d3e2c44708e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anxiety</th>\n",
       "      <td>5.511061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar</th>\n",
       "      <td>5.978278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depression</th>\n",
       "      <td>5.667380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>3.903474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality disorder</th>\n",
       "      <td>5.424887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.118644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stress</th>\n",
       "      <td>5.664143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suicidal</th>\n",
       "      <td>7.922907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          mean  min         max\n",
       "status                                         \n",
       "Anxiety               5.511061  0.0  100.000000\n",
       "Bipolar               5.978278  0.0   25.000000\n",
       "Depression            5.667380  0.0  100.000000\n",
       "Normal                3.903474  0.0  100.000000\n",
       "Personality disorder  5.424887  0.0   27.118644\n",
       "Stress                5.664143  0.0   57.142857\n",
       "Suicidal              7.922907  0.0  100.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_stats = data.groupby('status')['perc_upper_words'].agg(['mean', 'min', 'max'])\n",
    "upper_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ad87b-56f7-4d6e-95e2-2cd19e9932bc",
   "metadata": {},
   "source": [
    "Interesting. 'Normal' has the lowest average perc_upper_words with a mean of 3.90, whereas 'Suicidal' has the highest with a mean of 7.92.\n",
    "\n",
    "Let's see if there are any noticeable trends for the same metric applied to lower-case words. A higher percentage of lower-case words might be indicative of disorders like 'Depression'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1bfe5ba9-5c72-4558-b6c7-1b2f4482b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating perc_upper to calculate based on number of words\n",
    "def perc_lower_words(text): \n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    words = re.findall(r'\\b[a-zA-Z\\'-]+\\b', text) # Including apostrophes and hyphens\n",
    "    words = [word for word in words if not re.search(r'\\d', word)] # Filter out numbers\n",
    "    if not words:\n",
    "        return 0\n",
    "    lowercase_words = [word for word in words if word.islower()]\n",
    "    return len(lowercase_words) / len(words) * 100\n",
    "\n",
    "data['perc_lower_words'] = data['statement'].apply(perc_lower_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab3c0154-4d69-4988-8d8e-8a781a13d06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anxiety</th>\n",
       "      <td>87.107075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar</th>\n",
       "      <td>86.984639</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depression</th>\n",
       "      <td>90.887056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>89.098266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality disorder</th>\n",
       "      <td>87.738038</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stress</th>\n",
       "      <td>87.853044</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suicidal</th>\n",
       "      <td>87.430360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mean        min    max\n",
       "status                                           \n",
       "Anxiety               87.107075   0.000000  100.0\n",
       "Bipolar               86.984639  25.000000  100.0\n",
       "Depression            90.887056   0.000000  100.0\n",
       "Normal                89.098266   0.000000  100.0\n",
       "Personality disorder  87.738038  45.454545  100.0\n",
       "Stress                87.853044  14.285714  100.0\n",
       "Suicidal              87.430360   0.000000  100.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_stats = data.groupby('status')['perc_lower_words'].agg(['mean', 'min', 'max'])\n",
    "lower_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f4c6e-166a-434a-b75f-b2e6fbf6b989",
   "metadata": {},
   "source": [
    "'Depression' does indeed have the highest mean compared with our other 'status' values. \n",
    "\n",
    "However, typing in all lower-case is far more socially \"standard\" than typing in caps-lock. People often remove automatic capitilization as a setting on their phone or computer, which sets their default text to all lower-case. \n",
    "\n",
    "Given that lower-case typing is a societal 'norm', and a personal preference people often make regardless of psychological state...the 'lower_stats' numbers above do not vary drastically between 'status' groups enough for us to be able to use this as a predictive metric. \n",
    "\n",
    "We'll drop it and just keep perc_upper_words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8bc761d7-bcc9-4745-91b2-f48777f42ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['perc_lower_words'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685328a9-0982-4191-b269-f99749c0da88",
   "metadata": {},
   "source": [
    "#### Special Characters\n",
    "\n",
    "Let's explore if it will be useful to apply similar logic to special characters and punctuation. Let's examine all the different special characters in our entire dataframe, excluding numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a57d31ce-c74e-42e6-a1aa-f6afdf7df668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for special characters, excluding numbers\n",
    "def extract_special(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    return re.findall(r'[^\\w\\s\\d]', text)\n",
    "\n",
    "special_chars = data['statement'].apply(extract_special).explode().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "179ebfa3-9765-4751-bfbf-9a3f48e38287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, ',', '.', \"'\", '?', ':', ')', '(', '[', ']', '™', '‚', '=',\n",
       "       '\"', '/', '#', '!', '️', '±', '-', '…', '*', '”', '&', '€', '„',\n",
       "       '«', '–', '¤', '<', '°', '»', '^', ';', '\\u200b', '“', '+', '˜',\n",
       "       '‹', '¥', '©', '£', '@', '~', '¸', '—', '$', '§', '•', '¡', '❤',\n",
       "       '¨', '>', '‡', '|', '¯', '%', '¦', '{', '}', '’', '\\\\', '´', '`',\n",
       "       '†', '¶', '·', '®', '¢', '‰', '¿', '¬', '›', '👩', '\\u200d', '🎓',\n",
       "       '😇', '\\u200e', '➡', '💸', '😔', '🙂', '😦', '‘', '💕', '●', '🐰', '🥕',\n",
       "       '💖', '😱', '😐', '✌', '🏻', '🙃', '☹', '😅', '😢', '😂', '😩', '😖', '↑',\n",
       "       '👍', '🏼', '😭', '🤷', '♂', '⛅', '🛑', '🤦', '♀', '🙄', '😕', '😠', '😫',\n",
       "       '😁', '\\ufeff', '😒', '💩', '😜', '😨', '😆', '🏽', '😓', '\\x80', '\\x99',\n",
       "       '\\x9f', '\\x98', '\\xad', '\\x92', '\\x8f', '\\x8b', '\\x9c', '\\x9d',\n",
       "       '\\x8d', '\\x87', '\\x82', '\\x8a', '\\x81', '\\x8c', '\\x91', '\\x94',\n",
       "       '\\x84', '\\x95', '\\x96', '\\x89', '\\x83', '\\x88', '\\x97', '\\x90',\n",
       "       '\\x9a', '\\x8e', '\\U0001fae0', '🙏', '🔃', '💀', '🤣', '\\U0001fae3',\n",
       "       '😮', '💨', '😞', '🥰', '😳', '💜', '🖕', '\\U0001f979', '💅', '🙈', '🥱',\n",
       "       '🤩', '🖤', '😬', '😥', '🔥', '🤔', '❓', '⁉', '😏', '🤯', '😌', '⚡', '🏴',\n",
       "       '÷', '》', '×', '🐕', '😑', '📷', '⛽', '➲', '→', '\\U0001fae4', '👋',\n",
       "       '🥲', '🙁', '🤍', '🔴', '🌊', '💚', '✅', '✨', '😊', '🤡', '💗', '😍', '🧡',\n",
       "       '😚', '🌷', '\\u2060', '🙌', '‐', '\\U0001faf6', '🥺', '💝', '🙋', '⚠',\n",
       "       '☺', '😣', '😵', '💫', '😪', '♥', '🚩', '💛', '🤗', '👇', '🐟', '\\u202d',\n",
       "       '\\u202c', '🥳', '🤕', '💋', '🤭', '❌', '⬇', '💔', '🍃', '😀', '🤞', '☠',\n",
       "       '🤫', '😹', '\\U0001fae2', '⏳'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60997a5d-1207-4cd1-9dd8-9040b0cb0761",
   "metadata": {},
   "source": [
    "#### Emojis\n",
    "\n",
    "Let's try and categorize the emojis used. There are few enough that we can manually create lists identifying emojis we deem to carry clear 'positive' or 'negative' sentiments. We'll ignore any \"neutral\" emojis: we're about to get into more detailed and layered analysis of the actual text in a bit, so we don't need to waste time getting *too* intricate with these additional features!\n",
    "\n",
    "If we have to choose one or the other, we'll probably choose **negative** emojis as our main predictive metric (with regards to emoji sentiments)...since positive emojis could be used sarcastically. \n",
    "\n",
    "The sarcastic use of positive \"sentiments\" to indicate negativity is far more common than the sarcastic use of negative sentiments to indicate positivity. \n",
    "\n",
    "##### Varying Cultural Expression\n",
    "\n",
    "There can be differences in a person's interpretation and use of emojis when conveying sentiments, based on a variety of cultural factors. We don't know enough about the people who created this dataset, nor do we have demographic information on the users in the dataset itself, to make those determinations...We'll categorize emoji sentiments through our own cultural lens, and see how well our version of this metric helps or hurts our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "45bbcbb4-9a3a-4cf9-8f9b-4427620dc4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emojis = ['😇', '💖', '🙂', '💕', '❤', '😅', '😂', '👍', '😁', '😆', '🤣', '🥰', '💜', '🤩', '🖤', '😌', '🥲', '🤍','💚','😊',\n",
    "              '💗', '😍', '🧡', '💝', '♥', '💛', '🤗', '😀', '😹', '😏']\n",
    "\n",
    "neg_emojis = ['😔', '😦', '😐', '😢', '😩', '😖', '😭', '🤦', '🙄', '😕', '😠', '😫', '😒', '😨', '😓', '😮', '😞', '😳', '😥',\n",
    "              '😑', '🙁', '😣', '😪', '🤕', '💔']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730721c1-6a86-4236-bc2b-5aea55bf558d",
   "metadata": {},
   "source": [
    "##### Binary Classification\n",
    "\n",
    "As we stated, we don't need to get too intricate. Let's create a binary classifier to simply determine whether a 'statement' has any number of positive or negative emojis in it's text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b17622ad-b795-401e-88d7-17bc76fe65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 if any positive emojis in 'statement', else 0\n",
    "def has_pos_emoji(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return 1 if any(emoji in text for emoji in pos_emojis) else 0\n",
    "\n",
    "data['pos_emoji'] = data['statement'].apply(has_pos_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "519aa4e0-b1dd-41e2-8b29-3527a8b92a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for negative emojis\n",
    "def has_neg_emoji(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return 1 if any(emoji in text for emoji in neg_emojis) else 0\n",
    "\n",
    "data['neg_emoji'] = data['statement'].apply(has_neg_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee22008a-d237-4b6c-b2fe-22c8fe66f5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows with POSITIVE Emojis:\n",
      "status\n",
      "Anxiety                 39\n",
      "Bipolar                 17\n",
      "Depression              10\n",
      "Normal                  15\n",
      "Personality disorder    22\n",
      "Stress                   7\n",
      "dtype: int64\n",
      "\n",
      "Number of Rows with NEGATIVE Emojis:\n",
      "status\n",
      "Anxiety                 40\n",
      "Bipolar                 12\n",
      "Depression               8\n",
      "Personality disorder    14\n",
      "Stress                  13\n",
      "dtype: int64\n",
      "\n",
      "Number of Rows with BOTH:\n",
      "status\n",
      "Anxiety                 2\n",
      "Depression              2\n",
      "Personality disorder    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define a function to count rows with certain values\n",
    "def count_rows(df, pos_val, neg_val):\n",
    "    filtered_df = df[(df['pos_emoji'] == pos_val) & (df['neg_emoji'] == neg_val)]\n",
    "    return filtered_df.groupby('status').size()\n",
    "\n",
    "print(\"Number of Rows with POSITIVE Emojis:\")\n",
    "print(count_rows(data, 1, 0))\n",
    "print(\"\")\n",
    "print(\"Number of Rows with NEGATIVE Emojis:\")\n",
    "print(count_rows(data, 0, 1))\n",
    "print(\"\")\n",
    "print(\"Number of Rows with BOTH:\")\n",
    "print(count_rows(data, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b1701-20fc-43b0-b130-6e6446d6bafb",
   "metadata": {},
   "source": [
    "We were correct in assuming that the use of negative emojis might be a more telling metric than the use of positive emojis. There are no 'Normal' status users that have used any negative emojis whatsoever. The results for positive emojis are more ambiguous. Let's drop that column.\n",
    "\n",
    "(There are a suprisingly few amount of rows in our dataset that contain emojis. Our neg_emoji column is probably useless as well, but we'll keep it for right now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "63b6a8be-39d7-4700-9938-3f60dad94883",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['pos_emoji'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d07c2-88a8-43d7-8091-6c9fdaf5d7c5",
   "metadata": {},
   "source": [
    "#### ! and ?\n",
    "\n",
    "The last notable special characters we might want to document prior to text cleaning are exclamation points ('!') and question marks ('?'). These are frequently used to convey emotional sentiment, especially distress when used repeatedly (ex. '!!!!!', '????', '!?!?!?!?'). \n",
    "\n",
    "Let's add columns counting the number of times a person uses these in a 'statement'.\n",
    "\n",
    "**Normalization might not be useful here**. Using special characters such as '!' and '?' more than once is already noteworthy, especially in brief statements made on social media. Identifying an excess count, regardless of statement length, is probably our best metric. \n",
    "\n",
    "We'll try normalizing first, to see what the numbers look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "566c3282-3fb3-41a8-aa17-5eb385e854f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count characters excluding whitespace and calculate proportion of 'char'\n",
    "def calculate_char_ratio(text, char):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "\n",
    "    # Remove whitespace and count non-whitespace characters\n",
    "    non_whitespace_text = re.sub(r'\\s+', '', text)\n",
    "    total_characters = len(non_whitespace_text)\n",
    "    \n",
    "    # Count occurrences of 'char'\n",
    "    char_count = text.count(char)\n",
    "    \n",
    "    # Calculate proportion\n",
    "    if total_characters == 0:\n",
    "        return 0\n",
    "    ratio = char_count / total_characters\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "# Apply the function to calculate proportions for '!' and '?'\n",
    "data['exclamation_ratio'] = data['statement'].apply(lambda x: calculate_char_ratio(x, '!'))\n",
    "data['question_ratio'] = data['statement'].apply(lambda x: calculate_char_ratio(x, '?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0b3577c-9e21-4e8c-9bd2-21c5150e9b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of Exclamation Points by Status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anxiety</th>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar</th>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depression</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality disorder</th>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stress</th>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suicidal</th>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.246154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          mean  min       max\n",
       "status                                       \n",
       "Anxiety               0.000642  0.0  0.157895\n",
       "Bipolar               0.000562  0.0  0.056604\n",
       "Depression            0.000236  0.0  0.138298\n",
       "Normal                0.002042  0.0  0.740741\n",
       "Personality disorder  0.000318  0.0  0.026157\n",
       "Stress                0.000396  0.0  0.031008\n",
       "Suicidal              0.000417  0.0  0.246154"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing stats by 'status' group\n",
    "exclamation_stats = data.groupby('status')['exclamation_ratio'].agg(['mean', 'min', 'max'])\n",
    "print(\"Ratio of Exclamation Points by Status\")\n",
    "exclamation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4875b053-663f-4acb-92ce-6566c8f97f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of Question Marks by Status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anxiety</th>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar</th>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depression</th>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality disorder</th>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stress</th>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suicidal</th>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          mean  min       max\n",
       "status                                       \n",
       "Anxiety               0.002883  0.0  0.100000\n",
       "Bipolar               0.003458  0.0  0.090909\n",
       "Depression            0.001452  0.0  0.166667\n",
       "Normal                0.005444  0.0  0.466667\n",
       "Personality disorder  0.003138  0.0  0.062500\n",
       "Stress                0.001802  0.0  0.210811\n",
       "Suicidal              0.001741  0.0  0.111111"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_stats = data.groupby('status')['question_ratio'].agg(['mean', 'min', 'max'])\n",
    "print(\"Ratio of Question Marks by Status\")\n",
    "question_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059614c-1d5d-4c7b-b1c1-bc1ac96f01ab",
   "metadata": {},
   "source": [
    "Interesting. Just like we said, normalizing to account or statement length might not be ideal here. Our non-normalized counts might be easier to interpret. \n",
    "\n",
    "Let's run the same for merely the count of each character, not the ratio proportional to the total number of characters in the statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2fa1452c-655d-4899-b0a6-154fb201c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclamation Point Count by Status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anxiety</th>\n",
       "      <td>0.301355</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar</th>\n",
       "      <td>0.301479</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depression</th>\n",
       "      <td>0.110824</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>0.098635</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality disorder</th>\n",
       "      <td>0.178771</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stress</th>\n",
       "      <td>0.176625</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suicidal</th>\n",
       "      <td>0.116906</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          mean  min  max\n",
       "status                                  \n",
       "Anxiety               0.301355    0   27\n",
       "Bipolar               0.301479    0    9\n",
       "Depression            0.110824    0   39\n",
       "Normal                0.098635    0   20\n",
       "Personality disorder  0.178771    0   26\n",
       "Stress                0.176625    0   14\n",
       "Suicidal              0.116906    0   58"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to count occurrences of '!'\n",
    "def count_exclamation(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return text.count('!')\n",
    "\n",
    "# Add 'exc_count' column to the DataFrame\n",
    "data['exclamation_count'] = data['statement'].apply(count_exclamation)\n",
    "\n",
    "# Printing stats by 'status' group\n",
    "exccount_stats = data.groupby('status')['exclamation_count'].agg(['mean', 'min', 'max'])\n",
    "print(\"Exclamation Point Count by Status\")\n",
    "exccount_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "721b53ea-8bd0-42cc-9422-c71cdac6754d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Mark Count by Status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anxiety</th>\n",
       "      <td>1.056953</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar</th>\n",
       "      <td>1.565374</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depression</th>\n",
       "      <td>0.652151</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>0.170896</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality disorder</th>\n",
       "      <td>1.264804</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stress</th>\n",
       "      <td>0.648059</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suicidal</th>\n",
       "      <td>0.616389</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          mean  min  max\n",
       "status                                  \n",
       "Anxiety               1.056953    0   15\n",
       "Bipolar               1.565374    0   20\n",
       "Depression            0.652151    0   47\n",
       "Normal                0.170896    0    8\n",
       "Personality disorder  1.264804    0   22\n",
       "Stress                0.648059    0   39\n",
       "Suicidal              0.616389    0   34"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to count occurrences of '!'\n",
    "def count_question(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return text.count('?')\n",
    "\n",
    "# Add 'exc_count' column to the DataFrame\n",
    "data['question_count'] = data['statement'].apply(count_question)\n",
    "\n",
    "# Printing stats by 'status' group\n",
    "questcount_stats = data.groupby('status')['question_count'].agg(['mean', 'min', 'max'])\n",
    "print(\"Question Mark Count by Status\")\n",
    "questcount_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631284c-7cdc-4033-b9e0-f256fe116974",
   "metadata": {},
   "source": [
    "Yes!! These numbers are far more useful, and give us interesting insight that aligns with what we know about personality disorders. It makes a lot of sense that 'Suicidal' would have the highest maximum value of exclamation points...since an excess of exclamation points typically indicates high levels of distress or excitement. It also makes sense that 'Normal' has the lowest maximum value with regards to question mark counts...for similar reasons.\n",
    "\n",
    "We won't want to jump to concrete conclusions based on personal perception and prior knowledge...we'll trying running our model and see how well these columns actually correlate to and are predictive of 'status'!\n",
    "\n",
    "We'll keep these two new count features for now and drop our ratio columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2af864e2-007d-4e0f-beb7-69bef8c41303",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['exclamation_ratio', 'question_ratio'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "422b7963-aa96-4a63-807a-90a86b7ff602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_count\n",
       "0     35919\n",
       "1      8260\n",
       "2      3689\n",
       "3      1627\n",
       "4       680\n",
       "5       354\n",
       "6       212\n",
       "7       116\n",
       "8        66\n",
       "9        39\n",
       "10       31\n",
       "11       24\n",
       "12       12\n",
       "14        9\n",
       "16        7\n",
       "13        7\n",
       "15        3\n",
       "17        2\n",
       "18        2\n",
       "21        2\n",
       "19        2\n",
       "22        2\n",
       "23        2\n",
       "20        2\n",
       "31        1\n",
       "47        1\n",
       "34        1\n",
       "39        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking value_counts to see how useful new columns are\n",
    "data['question_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd80633c-141c-4959-87df-292bbe8b1e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exclamation_count\n",
       "0     47503\n",
       "1      2237\n",
       "2       666\n",
       "3       279\n",
       "4       164\n",
       "5        72\n",
       "6        46\n",
       "8        27\n",
       "7        19\n",
       "9        15\n",
       "14        7\n",
       "10        7\n",
       "12        7\n",
       "13        5\n",
       "16        4\n",
       "11        2\n",
       "24        2\n",
       "15        2\n",
       "20        2\n",
       "17        1\n",
       "18        1\n",
       "58        1\n",
       "19        1\n",
       "39        1\n",
       "26        1\n",
       "27        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['exclamation_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982628c8-900d-404c-bf6b-3ba42c45911b",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "Now let's clean our text by making everything lower-case, removing special characters, etc.\n",
    "\n",
    "##### Contractions\n",
    "\n",
    "We could include a step in our cleaning that prepares our text to analyze contractions (ex. \"don't\", \"aren't\", \"they're\") as *bigrams*. However there aren't any contraction words that are particuarly indicative of sentiment, especially with regards to personality disorders. We want to be detailed, without wasting time - so we will expand our contractions, but won't worry about providing any further analysis on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ed8763f2-2837-4b59-88ce-fc88808c48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions\n",
    "data['statement'] = data['statement'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7b9f48b0-12c5-4a0c-b847-bde473957c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    # Regex pattern to match emojis (covers a wide range of emojis)\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        '\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        '\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        '\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        '\\U0001F700-\\U0001F77F'  # alchemical symbols\n",
    "        '\\U0001F780-\\U0001F7FF'  # Geometric Shapes Extended\n",
    "        '\\U0001F800-\\U0001F8FF'  # Supplemental Arrows-C\n",
    "        '\\U0001F900-\\U0001F9FF'  # Supplemental Symbols and Pictographs\n",
    "        '\\U0001FA00-\\U0001FA6F'  # Chess Symbols\n",
    "        '\\U0001FA70-\\U0001FAFF'  # Symbols and Pictographs Extended-A\n",
    "        '\\U00002700-\\U000027BF'  # Dingbats\n",
    "        '\\U000024C2-\\U0001F251' \n",
    "        ']+', \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Define function to preprocess text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove links\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove special char's & punct\n",
    "    text = re.sub(r'\\[|\\]', '', text)  # Remove square brackets\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\n', '', text)  # Remove newlines\n",
    "    text = remove_emojis(text) # Apply emoji function from above\n",
    "    return text\n",
    "\n",
    "data['cleaned_statement'] = data['statement'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "544c742c-0949-46ba-b940-98fc9eede27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "      <th>perc_upper_words</th>\n",
       "      <th>neg_emoji</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>cleaned_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oh my gosh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trouble sleeping confused mind restless heart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all wrong back off dear forward doubt stay in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have shifted my focus to something else but ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i have shifted my focus to something else but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am restless and restless, it is been a month...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i am restless and restless it is been a month ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   status  \\\n",
       "0                                         oh my gosh  Anxiety   \n",
       "1  trouble sleeping, confused mind, restless hear...  Anxiety   \n",
       "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety   \n",
       "3  I have shifted my focus to something else but ...  Anxiety   \n",
       "4  I am restless and restless, it is been a month...  Anxiety   \n",
       "\n",
       "   perc_upper_words  neg_emoji  exclamation_count  question_count  \\\n",
       "0               0.0          0                  0               0   \n",
       "1               0.0          0                  0               0   \n",
       "2               0.0          0                  0               0   \n",
       "3               0.0          0                  0               0   \n",
       "4               0.0          0                  0               1   \n",
       "\n",
       "                                   cleaned_statement  \n",
       "0                                         oh my gosh  \n",
       "1  trouble sleeping confused mind restless heart ...  \n",
       "2  all wrong back off dear forward doubt stay in ...  \n",
       "3  i have shifted my focus to something else but ...  \n",
       "4  i am restless and restless it is been a month ...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7878dfe-f6ec-4276-b6ba-27173f2cb0a1",
   "metadata": {},
   "source": [
    "## Tokenization, Stop Words, Lemmatization\n",
    "\n",
    "Let's tokenize our data, remove stopwords, then use lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c3b33-14b1-45c2-92ca-b170b5e5b59b",
   "metadata": {},
   "source": [
    "### 'Not'\n",
    "\n",
    "Once we went ahead with tokenization, removal of stopwords, lemmatizaion, and printing a frequency distribution...some of the most-frequent tokens were words such as 'like', 'want' and 'know'...the meaning and interpretation of these words can be drastically altered depending on whether or not the word 'not' comes before them.\n",
    "\n",
    "Let's alter our code to make sure **'not'** is *excluded* from the stop words we remove. Then we can create ***bigrams*** such as ('not', 'like') and ('not', 'want') that might help us contextualize our tokens more specifically. We'll wait to revisit our bigrams later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "625ab26c-5328-4b34-92bd-1cd005f21e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of stop words\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# Excluding 'not'\n",
    "important_words = ['not']\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to tokenize, remove stopwords, and lemmatize\n",
    "def tokenize_stopwords_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = [w for w in tokens if w not in stopwords_list or w in important_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in cleaned_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Tokenize\n",
    "data['tokens'] = data['cleaned_statement'].apply(tokenize_stopwords_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "feb2280a-16c6-4196-aa9d-3a88182790f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "      <th>perc_upper_words</th>\n",
       "      <th>neg_emoji</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>cleaned_statement</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>[oh, gosh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trouble sleeping confused mind restless heart ...</td>\n",
       "      <td>[trouble, sleeping, confused, mind, restless, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all wrong back off dear forward doubt stay in ...</td>\n",
       "      <td>[wrong, back, dear, forward, doubt, stay, rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have shifted my focus to something else but ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i have shifted my focus to something else but ...</td>\n",
       "      <td>[shifted, focus, something, else, still, worried]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am restless and restless, it is been a month...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i am restless and restless it is been a month ...</td>\n",
       "      <td>[restless, restless, month, boy, mean]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   status  \\\n",
       "0                                         oh my gosh  Anxiety   \n",
       "1  trouble sleeping, confused mind, restless hear...  Anxiety   \n",
       "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety   \n",
       "3  I have shifted my focus to something else but ...  Anxiety   \n",
       "4  I am restless and restless, it is been a month...  Anxiety   \n",
       "\n",
       "   perc_upper_words  neg_emoji  exclamation_count  question_count  \\\n",
       "0               0.0          0                  0               0   \n",
       "1               0.0          0                  0               0   \n",
       "2               0.0          0                  0               0   \n",
       "3               0.0          0                  0               0   \n",
       "4               0.0          0                  0               1   \n",
       "\n",
       "                                   cleaned_statement  \\\n",
       "0                                         oh my gosh   \n",
       "1  trouble sleeping confused mind restless heart ...   \n",
       "2  all wrong back off dear forward doubt stay in ...   \n",
       "3  i have shifted my focus to something else but ...   \n",
       "4  i am restless and restless it is been a month ...   \n",
       "\n",
       "                                              tokens  \n",
       "0                                         [oh, gosh]  \n",
       "1  [trouble, sleeping, confused, mind, restless, ...  \n",
       "2  [wrong, back, dear, forward, doubt, stay, rest...  \n",
       "3  [shifted, focus, something, else, still, worried]  \n",
       "4             [restless, restless, month, boy, mean]  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffc78c-2866-4152-8a07-d8c58a9dba5c",
   "metadata": {},
   "source": [
    "# Creating bigrams with 'not'\n",
    "\n",
    "def bigrams_following_not(tokens):\n",
    "    bigram_list = list(bigrams(tokens))\n",
    "    filtered_bigrams = [bigram for bigram in bigram_list if bigram[0] == 'not']\n",
    "    return filtered_bigrams\n",
    "\n",
    "data['bigrams_following_not'] = data['tokens'].apply(bigrams_following_not)\n",
    "\n",
    "# Combine all bigrams across DataFrame\n",
    "all_bigrams = [bigram for bigram_list in data['bigrams_following_not'] for bigram in bigram_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb868a6b-76a8-4993-bcad-890df9030033",
   "metadata": {},
   "source": [
    "# Remove 'not' from tokens, now that we've made bigrams\n",
    "def remove_not(tokens):\n",
    "    return [token for token in tokens if token != 'not']\n",
    "\n",
    "# Apply function\n",
    "data['tokens'] = data['tokens'].apply(remove_not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbb821-3926-4aa5-941c-ea2c8838ed64",
   "metadata": {},
   "source": [
    "## Frequency Distribution by 'Status' Group\n",
    "\n",
    "Let's plot our frequency distributions for each 'status' group, to see which words are most commonly used for each personality disorder. \n",
    "\n",
    "We'll want to make use of common bigrams, to see how they compare to our single-word tokens with regards to frequency. We tried combining our tokens with common bigrams and plotting the frequency of both for each status...however our bigrams didn't seem to have as common frequency distribution, and therefore didn't appear on our graphs. \n",
    "\n",
    "We'll plot them separately for now, and keep the idea of combining their distributions in the back of our minds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e32e73-f6d3-4e5b-b345-86a18469004c",
   "metadata": {},
   "source": [
    "# Group by 'status' and aggregate tokens\n",
    "status_groups = data.groupby('status')['tokens'].sum()\n",
    "\n",
    "# Function to get the most common words for each group\n",
    "def get_most_common_words(tokens, num_common=10):\n",
    "    freqdist = FreqDist(tokens)\n",
    "    return freqdist.most_common(num_common)\n",
    "\n",
    "# Apply frequency distribution calculation for each status\n",
    "most_common_words_by_status = status_groups.apply(lambda x: get_most_common_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d33d78-efc7-4dca-9793-09eb535d3059",
   "metadata": {},
   "source": [
    "#### Normalize Frequency Distribution Counts\n",
    "\n",
    "Because our dataframe is so large, the code takes quite a while to run. Therefore we decided it's better to test things out, then go back and edit our previous code...to decrease the size and computational runttime of our notebook. \n",
    "\n",
    "We tried normalizing our frequency distributions prior to plotting, since we saw in one of the first iterations of this code that they cover a wide range of numbers. We used *log normalization*, since our values cover a wide range and will also be susceptible to outliers.\n",
    "\n",
    "However, our graphs with log normalization were not very useful to interpret. It might actually be better to get a sense of *how* varied the distribution counts are among words first, *then* normalize if that's helpful.\n",
    "\n",
    "We will **not** normalize our counts for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497caab-f3e7-4ca0-afa5-f1e72bd7d7ba",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "# Set up figure and axes\n",
    "fig, axes = plt.subplots(nrows=7, figsize=(12, 12))\n",
    "\n",
    "# Empty dict to hold words that have already been plotted and their colors\n",
    "plotted_words_and_colors = {}\n",
    "# Establish color palette to pull from\n",
    "color_palette = sns.color_palette('cividis', n_colors=38)\n",
    "\n",
    "# Creating a plot for each unique status\n",
    "status_groups = data.groupby('status')\n",
    "for idx, (status, status_df) in enumerate(status_groups):\n",
    "    # Combine all tokens in the current status group\n",
    "    all_tokens_in_status = status_df['tokens'].explode()\n",
    "    \n",
    "    # Calculate frequency distribution\n",
    "    freq_dist = FreqDist(all_tokens_in_status)\n",
    "    \n",
    "    # Get the top 10 tokens and their frequencies\n",
    "    top_10 = freq_dist.most_common(10)\n",
    "    tokens, counts = zip(*top_10)\n",
    "    \n",
    "    # Select appropriate colors, reusing colors if tokens repeat\n",
    "    colors = []\n",
    "    for token in tokens:\n",
    "        if token not in plotted_words_and_colors:\n",
    "            new_color = color_palette.pop(0)\n",
    "            plotted_words_and_colors[token] = new_color\n",
    "        colors.append(plotted_words_and_colors[token])\n",
    "    \n",
    "    # Select axes, plot data, set title\n",
    "    ax = axes[idx]\n",
    "    bars = ax.bar(tokens, counts, color=colors)\n",
    "    ax.set_title(f\"Status: {status}\")\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.set_xticks(tokens)\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79494a-8f74-43e0-bcf1-02fd7ad5adb6",
   "metadata": {},
   "source": [
    "## Dropping Common Tokens\n",
    "\n",
    "We need to filter our tokens further. We have a lot of overlap of frequent tokens among our status groups which makes sense...tokens such as 'like' are often used as colloquial \"filler-words\" with no substantial meaning, tokens such as 'get', 'would', and 'know' *could* be useful...but they're also common words used in every-day speech that have no meaning out of context, *especially* when they also appear across all of our status groups.\n",
    "\n",
    "We need to find a way to identify common tokens specific to a singular status group, that will help us better predict features unique to that status group alone.\n",
    "\n",
    "Let's define a **threshold** the filters out tokens appearing a certain number of times across all 'statement's in our dataframe. We'll start with 70%, and go from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8678a71-01fc-4580-af6e-c90ee124db6d",
   "metadata": {},
   "source": [
    "# Combine all tokens across the DataFrame\n",
    "all_tokens = [token for token_list in data['tokens'] for token in token_list]\n",
    "\n",
    "# Calculate frequency distribution of all tokens\n",
    "freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "# Set a frequency threshold of 70%\n",
    "threshold = 0.7 * len(data)\n",
    "\n",
    "# Identify common tokens\n",
    "common_tokens = {token for token, count in freq_dist.items() if count > threshold}\n",
    "\n",
    "# Function to remove common tokens\n",
    "def remove_common_tokens(tokens):\n",
    "    return [token for token in tokens if token not in common_tokens]\n",
    "\n",
    "# Apply filtering to the tokens column\n",
    "data['filtered_tokens'] = data['tokens'].apply(remove_common_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a173a-27ce-48be-9992-ce05c9ae64f8",
   "metadata": {},
   "source": [
    "# Group by 'status' and aggregate filtered tokens\n",
    "status_groups_filtered = data.groupby('status')['filtered_tokens'].sum()\n",
    "\n",
    "# Function to get the most common words for each group\n",
    "def get_most_common_words(tokens, num_common=10):\n",
    "    freqdist = FreqDist(tokens)\n",
    "    return freqdist.most_common(num_common)\n",
    "\n",
    "# Apply frequency distribution calculation for each status\n",
    "most_common_words_by_status_filtered = status_groups_filtered.apply(lambda x: get_most_common_words(x))\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, axes = plt.subplots(nrows=7, figsize=(12, 12))\n",
    "\n",
    "# Empty dict to hold words that have already been plotted and their colors\n",
    "plotted_words_and_colors = {}\n",
    "# Establish color palette to pull from\n",
    "color_palette = sns.color_palette('cividis', n_colors=38)\n",
    "\n",
    "# Creating a plot for each unique status\n",
    "for idx, (status, words_list) in enumerate(most_common_words_by_status_filtered.items()):\n",
    "    # Get the top words and their frequencies\n",
    "    top_words = words_list\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    # Select appropriate colors, reusing colors if words repeat\n",
    "    colors = []\n",
    "    for word in words:\n",
    "        if word not in plotted_words_and_colors:\n",
    "            new_color = color_palette.pop(0)\n",
    "            plotted_words_and_colors[word] = new_color\n",
    "        colors.append(plotted_words_and_colors[word])\n",
    "    \n",
    "    # Select axes, plot data, set title\n",
    "    ax = axes[idx]\n",
    "    ax.bar(words, counts, color=colors)\n",
    "    ax.set_title(f\"Status: {status}\")\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.set_xticks(range(len(words)))\n",
    "    ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4eff2-b35a-4b9a-abb8-6fa921f6112d",
   "metadata": {},
   "source": [
    "This did seem to help a bit. For example, 'pain' was added to our top 10 most-common words for users with 'Anxiety', which is far more useful than the word 'like'. \n",
    "\n",
    "However we still have a lot of overlap. Let's adjust our threshold further and try 60%. We can always go back and adjust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3897def-d4c5-498d-8181-ec09c18519df",
   "metadata": {},
   "source": [
    "# Set a frequency threshold of 60%\n",
    "threshold = 0.6 * len(data)\n",
    "\n",
    "# Identify common tokens\n",
    "common_tokens = {token for token, count in freq_dist.items() if count > threshold}\n",
    "\n",
    "# Function to remove common tokens\n",
    "def remove_common_tokens(tokens):\n",
    "    return [token for token in tokens if token not in common_tokens]\n",
    "\n",
    "# Apply filtering to the tokens column\n",
    "data['filtered_tokens'] = data['tokens'].apply(remove_common_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3d801-d64f-4a73-91ea-6ea549941753",
   "metadata": {},
   "source": [
    "# Group by 'status' and aggregate filtered tokens\n",
    "status_groups_filtered = data.groupby('status')['filtered_tokens'].sum()\n",
    "\n",
    "# Function to get the most common words for each group\n",
    "def get_most_common_words(tokens, num_common=10):\n",
    "    freqdist = FreqDist(tokens)\n",
    "    return freqdist.most_common(num_common)\n",
    "\n",
    "# Apply frequency distribution calculation for each status\n",
    "most_common_words_by_status_filtered = status_groups_filtered.apply(lambda x: get_most_common_words(x))\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, axes = plt.subplots(nrows=7, figsize=(12, 12))\n",
    "\n",
    "# Empty dict to hold words that have already been plotted and their colors\n",
    "plotted_words_and_colors = {}\n",
    "# Establish color palette to pull from\n",
    "color_palette = sns.color_palette('cividis', n_colors=38)\n",
    "\n",
    "# Creating a plot for each unique status\n",
    "for idx, (status, words_list) in enumerate(most_common_words_by_status_filtered.items()):\n",
    "    # Get the top words and their frequencies\n",
    "    top_words = words_list\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    # Select appropriate colors, reusing colors if words repeat\n",
    "    colors = []\n",
    "    for word in words:\n",
    "        if word not in plotted_words_and_colors:\n",
    "            new_color = color_palette.pop(0)\n",
    "            plotted_words_and_colors[word] = new_color\n",
    "        colors.append(plotted_words_and_colors[word])\n",
    "    \n",
    "    # Select axes, plot data, set title\n",
    "    ax = axes[idx]\n",
    "    ax.bar(words, counts, color=colors)\n",
    "    ax.set_title(f\"Status: {status}\")\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.set_xticks(range(len(words)))\n",
    "    ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958873d-ef35-4520-a529-ce97127b0736",
   "metadata": {},
   "source": [
    "Super helpful! Notice how 'anymore' was added to 'Suicidal', and 'friend' was added to 'Personality disorder'.\n",
    "\n",
    "Let's try it one more time, and go quite extreme. We tried filtering again to 50%, and it didn't make much of a difference. Let's set our threshold at 35% as an experiment and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8b229-f112-48ae-91bb-e9921efa0868",
   "metadata": {},
   "source": [
    "# Set a frequency threshold of 35%\n",
    "threshold = 0.35 * len(data)\n",
    "\n",
    "# Identify common tokens\n",
    "common_tokens = {token for token, count in freq_dist.items() if count > threshold}\n",
    "\n",
    "# Function to remove common tokens\n",
    "def remove_common_tokens(tokens):\n",
    "    return [token for token in tokens if token not in common_tokens]\n",
    "\n",
    "# Apply filtering to the tokens column\n",
    "data['filtered_tokens'] = data['tokens'].apply(remove_common_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b621191-72fc-45cc-a009-fc17bff1f503",
   "metadata": {},
   "source": [
    "# Group by 'status' and aggregate filtered tokens\n",
    "status_groups_filtered = data.groupby('status')['filtered_tokens'].sum()\n",
    "\n",
    "# Function to get the most common words for each group\n",
    "def get_most_common_words(tokens, num_common=10):\n",
    "    freqdist = FreqDist(tokens)\n",
    "    return freqdist.most_common(num_common)\n",
    "\n",
    "# Apply frequency distribution calculation for each status\n",
    "most_common_words_by_status_filtered = status_groups_filtered.apply(lambda x: get_most_common_words(x))\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, axes = plt.subplots(nrows=7, figsize=(12, 12))\n",
    "\n",
    "# Empty dict to hold words that have already been plotted and their colors\n",
    "plotted_words_and_colors = {}\n",
    "# Establish color palette to pull from\n",
    "color_palette = sns.color_palette('cividis', n_colors=38)\n",
    "\n",
    "# Creating a plot for each unique status\n",
    "for idx, (status, words_list) in enumerate(most_common_words_by_status_filtered.items()):\n",
    "    # Get the top words and their frequencies\n",
    "    top_words = words_list\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    # Select appropriate colors, reusing colors if words repeat\n",
    "    colors = []\n",
    "    for word in words:\n",
    "        if word not in plotted_words_and_colors:\n",
    "            new_color = color_palette.pop(0)\n",
    "            plotted_words_and_colors[word] = new_color\n",
    "        colors.append(plotted_words_and_colors[word])\n",
    "    \n",
    "    # Select axes, plot data, set title\n",
    "    ax = axes[idx]\n",
    "    ax.bar(words, counts, color=colors)\n",
    "    ax.set_title(f\"Status: {status}\")\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.set_xticks(range(len(words)))\n",
    "    ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bbdd8b-3254-47a3-8320-9a9b77b6d16e",
   "metadata": {},
   "source": [
    "Super interesting and potentially very useful!\n",
    "\n",
    "## Bigrams\n",
    "\n",
    "Let's revisit bigrams. We kept the stop word 'not' in our tokens, in order to see whether it's particularly useful in defining the sentiment of a following word in a bigram. \n",
    "\n",
    "We can furthermore calculate the **Raw Frequency** of bigrams with other tokens in our dataset as well as the **Pointwise Mutual Information Score** between bigram pairs in our dataset. Mutual Information Score essentially tells us the mutual dependence between two words. \n",
    "\n",
    "We can then determine whether any other bigram pairs are important. \n",
    "\n",
    "We'll perform this on our original 'tokens' column, rather than the lists of tokens filtered with thresholds. There may be some very common words that become unique to a particular status, when paired with another token in a bigram!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ba955-30f2-4bc3-89c5-82b781f44217",
   "metadata": {},
   "source": [
    "### Filtering out Noise\n",
    "\n",
    "It might be helpful to filter out pairs of words that occur frequently across our entire dataset, regardless of 'status' group. The same way we eliminate stop words from our tokens prior to drawing any meaningful conclusions.\n",
    "\n",
    "By first filtering out the most frequent bigrams in our dataset, we can help eliminate unecessary noise.\n",
    "\n",
    "To improve computational efficiency, we ran some code already and have some interesting takeaways that we can apply here prior to re-running...which will make our results more useful and improve run-times. \n",
    "\n",
    "We ran code filtering through the raw frequencies and PMI scores for the top 50 bigrams in the entire dataset, as well as by status group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd84a8-8438-4354-9dba-1fc21b09b5e1",
   "metadata": {},
   "source": [
    "### 'Not' and 'Like'\n",
    "\n",
    "Our main takeaways are regarding the words **not** and **like**. \n",
    "\n",
    "#### Like\n",
    "\n",
    "As we mentioned earlier, **like** is a pretty useless filler word that's only important if it's conveying some type of sentiment in it's verb form (\"to like\"). Once we looked at the most common bigrams, we found this was *not* the case in our dataset \n",
    "\n",
    "Ex. If the bigrams had included phrases such as ('i', 'like') or ('not', 'like') or ('you', 'like') in relation to specific 'status' groups, they might have been useful in predicting a person's mental state). These pairings did **not** appear in our results, so we can safely *drop the token 'like' from our entire dataframe*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f192e43-491f-461a-8bbb-7d0696d79e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'like' from tokens, now that we've made bigrams\n",
    "def remove_like(tokens):\n",
    "    return [token for token in tokens if token != 'like']\n",
    "\n",
    "# Apply function\n",
    "data['tokens'] = data['tokens'].apply(remove_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a0b14-1267-4178-8388-45dde8cc227c",
   "metadata": {},
   "source": [
    "#### Not\n",
    "\n",
    "Our results related to **not** were more nuanced. Some bigrams containing 'not' definitely *do* convey interesting and informative sentiments...however they appear so frequently across our entire dataframe, *and* all or most of our 'status' groups...that they really will not be helpful predicting specific personality disorders. \n",
    "\n",
    "We carefully filtered through the raw frequencies and PMI scores for the top 50 bigrams in the entire dataset, as well as by status group...and we retrieved certain 'not' bigrams that we think will be useful to include. Some were found frequently across multiple 'status' groups in our dataset, that might still be informative when breaking down their frequency counts by status in a further filtered dataframe...others were meaningful bigrams found frequently in individual 'status' groups alone.\n",
    "\n",
    "There's a list of our selects below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "41d0e6ff-1a24-4081-a49b-8008020aaba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of bigrams to keep\n",
    "bigrams_to_keep = [('could', 'not'), ('not', 'stop'), ('not', 'able'), ('not', 'help'), ('not', 'take'), ('not', 'seem'), ('not', 'work'), \n",
    "                   ('life', 'not'), ('not', 'see'), ('people', 'not'), ('not', 'anything'), ('not', 'understand'), ('not', 'good'), ('not', 'going'), \n",
    "                   ('not', 'sleep'), ('not', 'afford'), ('anymore', 'not'), ('die', 'not'), ('not', 'anymore'), ('would', 'not')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161d858-3101-42ea-a370-d2e4f880ad51",
   "metadata": {},
   "source": [
    "We're running into problems with accidentally creating or removing duplicate tokens, which might be a problem when we get to count vectorization shortly. \n",
    "\n",
    "We'll create additional lists below, which will help us determine whether to drop 'not' as a token from our 'tokens' list...while preserving the structure of all other tokens in the row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c681f939-c059-463c-8c8b-32848b18b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'like' from tokens, now that we've made bigrams\n",
    "def remove_not(tokens):\n",
    "    return [token for token in tokens if token != 'not']\n",
    "\n",
    "# Apply function\n",
    "data['tokens'] = data['tokens'].apply(remove_not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152772d-1dec-4469-a2d7-d600ef55ad65",
   "metadata": {},
   "source": [
    "# words where 'not' is second word in bigram selects\n",
    "not_second = ['could', 'life', 'people', 'anymore', 'die', 'would']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a55508-dd5b-4588-b362-25637d029ca8",
   "metadata": {},
   "source": [
    "# words where 'not' is first word in bigram selects\n",
    "not_first = ['stop', 'able', 'help', 'take', 'seem', 'work', 'see', 'anything', 'understand', 'good', 'going', 'sleep',\n",
    "             'afford', 'anymore']\n",
    "\n",
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Define function to remove 'not' tokens based on conditions\n",
    "def remove_not(tokens, not_first):\n",
    "    # Convert tokens to bigrams\n",
    "    bigram_list = list(bigrams(tokens))\n",
    "    \n",
    "    # Determine indices of 'not' to remove\n",
    "    remove_not_indices = set()\n",
    "    \n",
    "    # Identify indices where 'not' should be removed\n",
    "    for i, bigram in enumerate(bigram_list):\n",
    "        if bigram[0] == 'not' and bigram[1] not in not_first:\n",
    "            # If 'not' is the first word and the second word is NOT in 'not_first', mark 'not' for removal\n",
    "            remove_not_indices.add(i)\n",
    "        elif bigram[0] == 'not' and bigram[1] in not_first:\n",
    "            # If 'not' is the first word and the second word is in 'not_first', keep 'not'\n",
    "            # Remove index from removal set if it exists\n",
    "            remove_not_indices.discard(i)\n",
    "    \n",
    "    # Create a new list of tokens excluding the 'not' tokens that are to be removed\n",
    "    filtered_tokens = [token for i, token in enumerate(tokens) if not (token == 'not' and (i-1) in remove_not_indices)]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "data['tokens'] = data['tokens'].apply(lambda x: remove_not(x, not_first))\n",
    "\n",
    "# Verify the changes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385abf0-073d-4c35-86b4-82631eeb14cf",
   "metadata": {},
   "source": [
    "Now we run our code calculating the frequency and PMI scores for bigrams by 'status', with our filtered data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5cba3-37f3-4f59-b516-800c6a2b3908",
   "metadata": {},
   "source": [
    "#### Raw Frequency by Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcefebb-fed7-448a-9b26-b382bbb6b632",
   "metadata": {},
   "source": [
    "# Define a function to score bigrams\n",
    "def score_bigrams(tokens):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    bigrams_scored = bigram_finder.score_ngrams(BigramAssocMeasures.raw_freq)\n",
    "    return bigrams_scored[:50]  # Return top 30 scored bigrams\n",
    "\n",
    "# Group by 'status' and apply the scoring function\n",
    "bigrams_by_status = data.groupby('status')['tokens'].apply(lambda x: score_bigrams(x.sum()))\n",
    "\n",
    "# Print or access the results for each status\n",
    "for status, bigrams in bigrams_by_status.items():\n",
    "    print(f\"Status: {status}\")\n",
    "    for idx, (bigram, score) in enumerate(bigrams, 1):\n",
    "        print(f\"{idx}. {bigram}: {score}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbf44d-704b-448a-b34e-989b1a59e332",
   "metadata": {},
   "source": [
    "#### Mutual Information Score by Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6e4b7-0372-44f9-9632-139deba40a05",
   "metadata": {},
   "source": [
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "# Define a function to score bigrams using PMI\n",
    "def score_bigrams(tokens):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    bigram_finder.apply_freq_filter(20)  # Higher filter for larger dataframe\n",
    "    bigrams_scored = bigram_finder.score_ngrams(bigram_measures.pmi)\n",
    "    return bigrams_scored\n",
    "\n",
    "# Group by 'status' and apply the scoring function\n",
    "bigrams_by_status = data.groupby('status')['tokens'].apply(lambda x: score_bigrams(sum(x, [])))\n",
    "\n",
    "# Print or access the results for each status\n",
    "for status, bigrams in bigrams_by_status.items():\n",
    "    print(f\"Status: {status}\")\n",
    "    for idx, (bigram, score) in enumerate(bigrams[:50], 1):  # Print top 30\n",
    "        print(f\"{idx}. {bigram}: {score}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498ce82-95cf-4ad8-bce5-749ebf3bf69a",
   "metadata": {},
   "source": [
    "There is a lot of really fascinating and informative findings in our exploration of bigrams. \n",
    "\n",
    "For example, 'Anxiety' users appear to speak about physical health ailments and concerns (such as brain tumors, lymph nodes, heart attacks), which makes a lot of sense. 'Bipolar' users speak in 'up & down', 'high & low', 'mood swings'...which also makes sense. 'Depression' users conjure \"downward\" imagery of 'rock bottom', 'rabbit hole', etc. \n",
    "\n",
    "This is all super helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a1f4e4-092c-4852-88ad-fa3e5d47a0e2",
   "metadata": {},
   "source": [
    "# Combining tokens and bigrams\n",
    "def combine_tokens_bigrams(row):\n",
    "    tokens = row['tokens']\n",
    "    bigrams = row['bigrams_following_not']\n",
    "    return tokens + bigrams\n",
    "\n",
    "data['combined_tokens_bigrams'] = data.apply(combine_tokens_bigrams, axis=1)\n",
    "\n",
    "# Group by status and aggregate\n",
    "status_groups = data.groupby('status')['combined_tokens_bigrams'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2d08a-e902-4b78-b876-763ecd69e1d6",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "Let's move ahead with our train test split and begin vectorization of a model.\n",
    "\n",
    "We'll use **TF-IDF** vectorization, and we'll compare token frequencies not only to their total frequencies across documents/'statement' values...but across entire 'status' groups as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53b174-0716-4198-849d-96e0e869d97d",
   "metadata": {},
   "source": [
    "Let's make a new dataframe 'filtered_df', that extracts only the necessary features we need for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "62299bec-2c6d-4027-90c1-8f4f8632d134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "      <th>perc_upper_words</th>\n",
       "      <th>neg_emoji</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>cleaned_statement</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>[oh, gosh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trouble sleeping confused mind restless heart ...</td>\n",
       "      <td>[trouble, sleeping, confused, mind, restless, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all wrong back off dear forward doubt stay in ...</td>\n",
       "      <td>[wrong, back, dear, forward, doubt, stay, rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have shifted my focus to something else but ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i have shifted my focus to something else but ...</td>\n",
       "      <td>[shifted, focus, something, else, still, worried]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am restless and restless, it is been a month...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i am restless and restless it is been a month ...</td>\n",
       "      <td>[restless, restless, month, boy, mean]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   status  \\\n",
       "0                                         oh my gosh  Anxiety   \n",
       "1  trouble sleeping, confused mind, restless hear...  Anxiety   \n",
       "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety   \n",
       "3  I have shifted my focus to something else but ...  Anxiety   \n",
       "4  I am restless and restless, it is been a month...  Anxiety   \n",
       "\n",
       "   perc_upper_words  neg_emoji  exclamation_count  question_count  \\\n",
       "0               0.0          0                  0               0   \n",
       "1               0.0          0                  0               0   \n",
       "2               0.0          0                  0               0   \n",
       "3               0.0          0                  0               0   \n",
       "4               0.0          0                  0               1   \n",
       "\n",
       "                                   cleaned_statement  \\\n",
       "0                                         oh my gosh   \n",
       "1  trouble sleeping confused mind restless heart ...   \n",
       "2  all wrong back off dear forward doubt stay in ...   \n",
       "3  i have shifted my focus to something else but ...   \n",
       "4  i am restless and restless it is been a month ...   \n",
       "\n",
       "                                              tokens  \n",
       "0                                         [oh, gosh]  \n",
       "1  [trouble, sleeping, confused, mind, restless, ...  \n",
       "2  [wrong, back, dear, forward, doubt, stay, rest...  \n",
       "3  [shifted, focus, something, else, still, worried]  \n",
       "4             [restless, restless, month, boy, mean]  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5f60b6ef-2f74-48a9-a7e2-5d7d0bba0b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>perc_upper_words</th>\n",
       "      <th>neg_emoji</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[oh, gosh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[trouble, sleeping, confused, mind, restless, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[wrong, back, dear, forward, doubt, stay, rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[shifted, focus, something, else, still, worried]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[restless, restless, month, boy, mean]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    status  perc_upper_words  neg_emoji  exclamation_count  question_count  \\\n",
       "0  Anxiety               0.0          0                  0               0   \n",
       "1  Anxiety               0.0          0                  0               0   \n",
       "2  Anxiety               0.0          0                  0               0   \n",
       "3  Anxiety               0.0          0                  0               0   \n",
       "4  Anxiety               0.0          0                  0               1   \n",
       "\n",
       "                                              tokens  \n",
       "0                                         [oh, gosh]  \n",
       "1  [trouble, sleeping, confused, mind, restless, ...  \n",
       "2  [wrong, back, dear, forward, doubt, stay, rest...  \n",
       "3  [shifted, focus, something, else, still, worried]  \n",
       "4             [restless, restless, month, boy, mean]  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_keep = ['status', 'perc_upper_words', 'neg_emoji', 'exclamation_count', 'question_count', 'tokens']\n",
    "filtered_df = data.loc[:, columns_to_keep]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0af72752-7176-462f-866f-5787b511ef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 51073 entries, 0 to 52840\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   status             51073 non-null  object \n",
      " 1   perc_upper_words   51073 non-null  float64\n",
      " 2   neg_emoji          51073 non-null  int64  \n",
      " 3   exclamation_count  51073 non-null  int64  \n",
      " 4   question_count     51073 non-null  int64  \n",
      " 5   tokens             51073 non-null  object \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef913e6-0038-49d9-af76-8600b5777766",
   "metadata": {},
   "source": [
    "### Filtering for Empty Tokens\n",
    "\n",
    "After all of our preprocessing and cleaning...we might have some rows that are essentially \"empty\", with 0 tokens. This could happen if a person's statement included only stop words, emoticons, punctuation, etc. \n",
    "\n",
    "We'll filter for these and set them to NaN. If there are relatively few, compared to the 51,000+ rows we currently have in our dataframe, we can feel good about simply dropping them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9c25dd2a-39e2-4e55-93a4-932e04e4d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 51073 entries, 0 to 52840\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   status             51073 non-null  object \n",
      " 1   perc_upper_words   51073 non-null  float64\n",
      " 2   neg_emoji          51073 non-null  int64  \n",
      " 3   exclamation_count  51073 non-null  int64  \n",
      " 4   question_count     51073 non-null  int64  \n",
      " 5   tokens             50955 non-null  object \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Set rows with empty 'tokens' lists to NaN\n",
    "filtered_df.loc[filtered_df['tokens'].apply(lambda x: len(x) == 0), 'tokens'] = np.nan\n",
    "\n",
    "# Optional: Print the DataFrame to verify\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "daa77a1c-704f-4a0b-b125-51842c622509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NaNs\n",
    "filtered_df = filtered_df.dropna(subset=['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ec43c-eb2f-4727-8a3a-01d12e3143f9",
   "metadata": {},
   "source": [
    "## Vectorize and Multinomial Naive Bayes Classifier\n",
    "\n",
    "For our first model, we'll try a mulltinomial naive bayes classifier. This type of model deals with text data alone, so we'll leave the other numeric features we added out of this train test split.\n",
    "\n",
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288a095-6fd1-4c8a-934c-5ed8062e2a68",
   "metadata": {},
   "source": [
    "# Convert token lists to strings\n",
    "filtered_df['joined_tokens'] = filtered_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Define features and target\n",
    "X = filtered_df['joined_tokens']\n",
    "y = filtered_df['status']  # 'status' is the target variable\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53b664-6d07-48d0-91c0-021a3c64c6ab",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "\n",
    "We can either vectorize our singular tokens as unigrams, or include bigrams in our analysis. \n",
    "\n",
    "Furthermore, our dataset is larger so a higher number of max_features might be better.\n",
    "\n",
    "We can test the best parameters for our model using GridSearchCv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de9064-62a2-40e6-8177-cb37002c6d73",
   "metadata": {},
   "source": [
    "#### Check for Class Imbalance\n",
    "\n",
    "Before we fit our model, we'll need to handle class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad42231-3490-467a-b9a6-4c8448fd62cb",
   "metadata": {},
   "source": [
    "# Assuming `y_train` contains the target labels\n",
    "class_distribution = y_train.value_counts(normalize=True)\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994d553-f176-4698-b3e8-69c380456e5d",
   "metadata": {},
   "source": [
    "#### Large Dataset - Class Imbalance, Feature Importance\n",
    "\n",
    "Because our dataset is larger, we'll try to account for class imbalance by adjusting the class weights with the MultinomialNB 'class_prior'. \n",
    "\n",
    "We'll also want to optimize computational runttimes, by narrowing our feature selection if possible...We tried using **SelectKBest** with a **chi-squared** test. This would evaluate the statistical importace of each feature, by measuring the independence between a feature and target variable.\n",
    "\n",
    "SelectKBest would be the most suitable option for a model like Multinomial Naive Bayes. We'll look at other tree-based models and options for feature selection in a bit.\n",
    "\n",
    "However, our scores were even *worse*, so we'll remove SelectKbest from our Pipeline for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "37326fcb-4833-4b47-b717-17912a785765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set class prior probabilities based on class distribution\n",
    "class_prior = [0.307895, 0.300998, 0.208843, 0.071885, 0.048419, 0.044438, 0.017523]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108196f-b34b-4ea6-9389-cdb65947f999",
   "metadata": {},
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [10000, 20000],  # Number of features to use in the TF-IDF Vectorizer\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Use unigrams and/or bigrams\n",
    "    'feature_selection__k': [1000, 5000, 'all'],\n",
    "    'clf__alpha': [0.1, 0.5, 1.0],  # Smoothing parameter for Naive Bayes\n",
    "    'clf__fit_prior': [True, False]  # Whether to learn class prior probabilities or use the ones provided\n",
    "}\n",
    "\n",
    "# Create a pipeline that first transforms data using TF-IDF and then applies Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # Placeholder for TF-IDF Vectorizer\n",
    "    ('feature_selection', SelectKBest(score_func=chi2)),\n",
    "    ('clf', MultinomialNB(class_prior=class_prior))  # Naive Bayes with class priors\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV with timeout for joblib workers\n",
    "timeout_in_seconds = 300  # Timeout set to 5 minutes (300 seconds)\n",
    "with parallel_backend('loky'):\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid, \n",
    "        cv=5,  # Number of folds for cross-validation\n",
    "        scoring='f1_weighted',  # F1 score, weighted for imbalanced classes\n",
    "        verbose=1,  # Print progress\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # After fitting, now you can access best parameters and scores\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best F1 score: {grid_search.best_score_}\")\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "    print(f\"Test set score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24106c9-e22c-4161-bef7-0c9cf6aebf37",
   "metadata": {},
   "source": [
    "# Assuming best_model is your best estimator from GridSearchCV\n",
    "tfidf_vectorizer = best_model.named_steps['tfidf']\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "# Inspect the shape of the transformed feature matrix\n",
    "print(f\"Shape of X_train_tfidf: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be715af-007b-4801-9a1d-799f032e93c0",
   "metadata": {},
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=best_model.classes_)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Optionally, print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=best_model.classes_)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668dd1c-5b65-46ed-bd22-0edd32e06247",
   "metadata": {},
   "source": [
    "These scores are not very good. Which is okay! We shall continue tuning!\n",
    "\n",
    "Stress and Suicidal cases seem particularly hard to accurately predict, with lower f1-scores. We will keep this in mind as we tune and test other models...in case this seems to be a trend. \n",
    "\n",
    "We can come back to this model and tune it to yield better results...but first let's test some other models and see if they perform better as a baseline.\n",
    "\n",
    "Before we move on, let's see whether there's any informative results on feature importance or irrelevancy, that we can highlight and compare with other models...We moved ahead and tried running other models like *XGBoost*, however they unsurprisingly didn't reach convergence due to extraneous computational runtimes. This makes sense, because our dataset is currently so large and complex with 7 target variables. We need to figure out how to narrow things down if possible.\n",
    "\n",
    "We can trying **making lists of most and least important features across models**...then we can cross-reference using these lists and figure out which tokens to drop and which to keep as we hone in on feature selection.\n",
    "\n",
    "## Feature Importance - MultinomialNB\n",
    "\n",
    "Multinomial Bayes doesn't extract feature importance like some of our other predictive models...but we can still get some good idea of feature relevancy using log probabilities and extracting feature names. This will tell us the log of the probability that a particular word ends up in a feature class.\n",
    "\n",
    "We'll have to extract from SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cff4f7-5405-4cd3-b63a-f4bbd365293c",
   "metadata": {},
   "source": [
    "# Accessing the TfidfVectorizer and SelectKBest from the pipeline\n",
    "tfidf_vectorizer = best_model.named_steps['tfidf']\n",
    "select_k_best = best_model.named_steps['feature_selection']  # Assuming 'feature_selection' is the name of SelectKBest step\n",
    "\n",
    "# Getting feature names (words) from TfidfVectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get indices of selected features from SelectKBest\n",
    "selected_feature_indices = select_k_best.get_support(indices=True)\n",
    "\n",
    "# Accessing MultinomialNB classifier from the pipeline\n",
    "clf = best_model.named_steps['clf']\n",
    "\n",
    "# Getting the log probabilities of features given each class\n",
    "log_probabilities = clf.feature_log_prob_\n",
    "\n",
    "# Determine top and bottom features for all classes combined and store in combined lists\n",
    "top_n = 50  # Number of top and bottom features to consider\n",
    "\n",
    "multinomialnb_top_features = set()  # Using set to ensure uniqueness\n",
    "multinomialnb_bottom_features = set()\n",
    "\n",
    "for i, class_label in enumerate(clf.classes_):\n",
    "    # Filter log_probabilities and feature_names with selected_feature_indices\n",
    "    class_log_probabilities = log_probabilities[i, selected_feature_indices]\n",
    "    class_feature_names = [feature_names[idx] for idx in selected_feature_indices]\n",
    "\n",
    "    # Sort indices based on log probabilities\n",
    "    top_features_idx = np.argsort(class_log_probabilities)[::-1][:top_n]\n",
    "    bottom_features_idx = np.argsort(class_log_probabilities)[:top_n]\n",
    "\n",
    "    # Retrieve top and bottom features\n",
    "    top_features = [class_feature_names[idx] for idx in top_features_idx]\n",
    "    bottom_features = [class_feature_names[idx] for idx in bottom_features_idx]\n",
    "\n",
    "    # Extend sets with top and bottom features\n",
    "    multinomialnb_top_features.update(top_features)\n",
    "    multinomialnb_bottom_features.update(bottom_features)\n",
    "\n",
    "# Convert sets back to lists\n",
    "multinomialnb_top_features = list(multinomialnb_top_features)\n",
    "multinomialnb_bottom_features = list(multinomialnb_bottom_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5f0cc-dd57-40ee-a32d-7537c0547165",
   "metadata": {},
   "source": [
    "multinomialnb_top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709738f-9b36-4f12-98d6-02ed7b9e21b4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "multinomialnb_bottom_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11524c5e-8be0-42a8-a7b5-b32a5812167d",
   "metadata": {},
   "source": [
    "# Print top 20 and bottom 20 features for each class individually\n",
    "for i, class_label in enumerate(clf.classes_):\n",
    "    # Filter log_probabilities and feature_names with selected_feature_indices\n",
    "    class_log_probabilities = log_probabilities[i, selected_feature_indices]\n",
    "    class_feature_names = [feature_names[idx] for idx in selected_feature_indices]\n",
    "\n",
    "    # Sort indices based on log probabilities\n",
    "    top_features_idx = np.argsort(class_log_probabilities)[::-1][:20]\n",
    "    bottom_features_idx = np.argsort(class_log_probabilities)[:20]\n",
    "\n",
    "    # Retrieve top and bottom features\n",
    "    top_features = [class_feature_names[idx] for idx in top_features_idx]\n",
    "    bottom_features = [class_feature_names[idx] for idx in bottom_features_idx]\n",
    "\n",
    "    print(f\"Top 20 features for class '{class_label}':\")\n",
    "    print(top_features)\n",
    "    print()\n",
    "\n",
    "    print(f\"Bottom 20 features for class '{class_label}':\")\n",
    "    print(bottom_features)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6045db6-64c7-4d3f-8014-603d1672c3d8",
   "metadata": {},
   "source": [
    "##### Dropping - 'know', 'told', 'might', 'feel'\n",
    "\n",
    "Already from these, we can gain some valuable insight into features we can drop. Although these selections were made by log probabilities, which don't directly tell us a certain correlation or impact a feature might have on predicting a given class...we can highlight some repetetive features that add nothing but noise to our data. \n",
    "\n",
    "**'know'** and **'told'** are two that might be helpful to drop. We'll keep these for now, since there's a *chance* they might implicate a unique mental state (ex. 'know' can help us determine whether or not a person feels grounded in reality 'i know', or lost 'i don't know'...'told' could help implicate whether someone feels unheard 'i told', or attacked 'they told me!'...it's unlikely we'll need them but we'll run some more models before coming to any concrete conclusion). \n",
    "\n",
    "**'feel'** can be dropped. It's been appearing as one of the most frequently used tokens in all of our outputs regarding feature/token importance...which makes sense given that this dataset is exploring internal emotional states of users. Furthermore, 'feel' doesn't give us any further insight...because whatever word follows it (ex 'i feel **stressed**', 'i feel **happy**', 'i feel **worried**') is the actually informative word, *and* will implicate the verb \"to feel\". \n",
    "\n",
    "**'might'** can also be dropped. It's a filler word that doesn't seem to be operating in bigrams in a way that helpfully implicates emotional uncertainty. There will be other more unique ways to determine uncertainty of mental state by class if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "affd60e7-d46f-4776-821b-4f88702e1a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50955 entries, 0 to 52840\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   status             50955 non-null  object \n",
      " 1   perc_upper_words   50955 non-null  float64\n",
      " 2   neg_emoji          50955 non-null  int64  \n",
      " 3   exclamation_count  50955 non-null  int64  \n",
      " 4   question_count     50955 non-null  int64  \n",
      " 5   tokens             50953 non-null  object \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Remove 'like' from tokens, now that we've made bigrams\n",
    "def remove_feel(tokens):\n",
    "    return [token for token in tokens if token != 'feel']\n",
    "\n",
    "def remove_might(tokens):\n",
    "    return [token for token in tokens if token!= 'might']\n",
    "\n",
    "# Apply function\n",
    "data['tokens'] = data['tokens'].apply(remove_feel)\n",
    "filtered_df['tokens'] = filtered_df['tokens'].apply(remove_feel)\n",
    "\n",
    "data['tokens'] = data['tokens'].apply(remove_might)\n",
    "filtered_df['tokens'] = filtered_df['tokens'].apply(remove_might)\n",
    "\n",
    "# Set rows with empty 'tokens' lists to NaN\n",
    "filtered_df.loc[filtered_df['tokens'].apply(lambda x: len(x) == 0), 'tokens'] = np.nan\n",
    "\n",
    "# Optional: Print the DataFrame to verify\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a3174974-3e32-4cea-81ae-63e17d8a4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NaNs\n",
    "filtered_df = filtered_df.dropna(subset=['tokens'])\n",
    "\n",
    "# Convert token lists to strings\n",
    "filtered_df['joined_tokens'] = filtered_df['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b25f0-5c6c-4004-b226-127a98d74ba1",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "We need to continue to hone and select our most important features, in order to hopefully build models with better scores that are less computationally expensive. \n",
    "\n",
    "# Multinomial Logistic Regression\n",
    "\n",
    "Let's see how multinomial logistic regression performs as a model, and extract relevant features like we did for Multinomial Naive Bayes.\n",
    "\n",
    "We'll deal with **class imbalance** by adjusting our **class weight hyperparameter**, rather than using a technique like SMOTE.\n",
    "\n",
    "### Including Numeric Features\n",
    "\n",
    "We'll be able to include the numeric features we calculated earlier on, in addition to our vectorized text data.\n",
    "\n",
    "### LabelEncoder()\n",
    "\n",
    "We'll need to use labelencoder() to encode our target variable classes, so they are integer values that the model can interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947535f9-5d23-4290-960f-33550fa2994a",
   "metadata": {},
   "source": [
    "X = filtered_df.drop(columns=['status', 'tokens'])\n",
    "y = filtered_df['status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "y_train = pd.Series(y_train_encoded)\n",
    "y_test = pd.Series(y_test_encoded)\n",
    "\n",
    "# Check encoded labels\n",
    "print(\"Encoded y_train:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"Encoded y_test:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Mapping from encoded labels to original labels\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"Label Mapping:\")\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e990b0-2050-42fb-b73b-98276fbba153",
   "metadata": {},
   "source": [
    "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "\n",
    "X_train_text = vectorizer.fit_transform(X_train['joined_tokens'])\n",
    "X_test_text = vectorizer.transform(X_test['joined_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ea755-da7f-43c3-8a4c-d5c08b58d792",
   "metadata": {},
   "source": [
    "# Scale numeric data\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform StandardScaler on numeric data\n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train[['perc_upper_words', 'neg_emoji', 'exclamation_count', 'question_count']])\n",
    "X_test_numeric_scaled = scaler.transform(X_test[['perc_upper_words', 'neg_emoji', 'exclamation_count', 'question_count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e51a3-af46-4902-8106-76a0d9251bec",
   "metadata": {},
   "source": [
    "X_train_combined = hstack([X_train_text, X_train_numeric_scaled])\n",
    "X_test_combined = hstack([X_test_text, X_test_numeric_scaled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b5c9f-7816-45c1-a563-1b00d173d64d",
   "metadata": {},
   "source": [
    "### Feature Importance - SelectFromModel, Random Forest\n",
    "\n",
    "To minimize computational expense, we need to hone our feature selection in our model once again. Instead of using SelectKBest this time with a chi-squared test, let's use **SelectFromModel** paired with **RandomForestClassifier()** to select predictive features. \n",
    "\n",
    "We'll use Random Forest as opposed to Grandient Boosting, since our dataset is quite large and Gradient Boosting might be more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fb4b1-44fe-4e3a-90a4-571f7207e338",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Example param_grid for GridSearchCV with SelectFromModel\n",
    "param_grid = {\n",
    "    'select__threshold': ['mean', 'median', '2.*mean'],\n",
    "    'clf__C': [0.1, 1.0, 10.0],\n",
    "    'clf__solver': ['lbfgs', 'saga'],\n",
    "    'clf__max_iter': [2000], # Balance computational cost with covergence needs for max_iter\n",
    "}\n",
    "\n",
    "# Example pipeline with SelectFromModel and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('select', SelectFromModel(estimator=RandomForestClassifier())),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', random_state=42)), # Removed multi_class to account for warning, multinomial is default\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    param_grid, \n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_combined, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_score = grid_search.score(X_test_combined, y_test)\n",
    "print(f\"Test set score: {test_score}\")\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240a435-b236-47a2-9441-76cc23c2156f",
   "metadata": {},
   "source": [
    "## Convergence Errors\n",
    "\n",
    "We've run our logistic regression model many times now, and continue to get errors regarding our max_iterations. We've tried multiple values, yet it seems this model might not be working. Our dataset is pretty large and complex, so logistic regression might simply not be the best model.\n",
    "\n",
    "We'll leave the code in here as markdowns, in case it's helpful to refer back to at some point. There's no use having it run unecessarily everytime we run our notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193cfb3-65fd-4ee9-ba3f-5833f5e1d5e6",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Let's try building a Random Forest model, which is known to handle non-linear complex relationships in large datasets quite well. Yet is less computationally expensive than a model like Gradient Boosting.\n",
    "\n",
    "We will not need to use a Label Encoder or Standard Scaler for Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "846b0313-ab5f-431b-bcb3-26e6238d61ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_df.drop(columns=['status', 'tokens'])\n",
    "y = filtered_df['status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "202f02e6-1611-43b0-af4d-01a0acb2f757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "\n",
    "X_train_text = vectorizer.fit_transform(X_train['joined_tokens'])\n",
    "X_test_text = vectorizer.transform(X_test['joined_tokens'])\n",
    "\n",
    "X_train_numeric = X_train[['perc_upper_words', 'neg_emoji', 'exclamation_count', 'question_count']].values\n",
    "X_test_numeric = X_test[['perc_upper_words', 'neg_emoji', 'exclamation_count', 'question_count']].values\n",
    "\n",
    "X_train_combined = hstack([X_train_text, X_train_numeric])\n",
    "X_test_combined = hstack([X_test_text, X_test_numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fee10fda-432d-4a46-a6c5-b0550046c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200, 300],\n",
    "    'clf__max_depth': [None, 10, 20],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', RandomForestClassifier(random_state=42, class_weight='balanced')),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f3338-8999-4b1c-8044-92e9a9d2fe7f",
   "metadata": {},
   "source": [
    "@profile\n",
    "def run_grid_search():\n",
    "    with parallel_backend('loky'):\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='f1_weighted',\n",
    "            verbose=0,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        # Fit GridSearchCV\n",
    "        grid_search.fit(X_train_combined, y_train)\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        test_score = grid_search.score(X_test_combined, y_test)\n",
    "        print(f\"Test set score: {test_score}\")\n",
    "\n",
    "        print(\"Best parameters:\", grid_search.best_params_)\n",
    "        print(\"Best F1 score:\", grid_search.best_score_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b992bc36-a4ac-4d3d-adeb-ef709646b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env-3.9/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m      3\u001b[0m     pipeline,\n\u001b[1;32m      4\u001b[0m     param_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Fit GridSearchCV\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[1;32m     15\u001b[0m test_score \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mscore(X_test_combined, y_test)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learn-env-3.9/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learn-env-3.9/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learn-env-3.9/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learn-env-3.9/lib/python3.10/site-packages/sklearn/model_selection/_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     )\n\u001b[0;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learn-env-3.9/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learn-env-3.9/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learn-env-3.9/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learn-env-3.9/lib/python3.10/site-packages/joblib/parallel.py:1753\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_retrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m         \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m         \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m-> 1753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m   1754\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1755\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with parallel_backend('loky'):\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        verbose=0,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train_combined, y_train)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_score = grid_search.score(X_test_combined, y_test)\n",
    "    print(f\"Test set score: {test_score}\")\n",
    "\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best F1 score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea099c3b-6a44-4631-b180-5b369d62aa14",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Let's try XGBoost, which is known to yield extraodinary results on large, complex datasets that require multi-categorical targets.\n",
    "\n",
    "Let's train, test, split.\n",
    "\n",
    "We can **include our numeric features** this time, in addition to our text data which will be vectorized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d33293-c1e7-4a63-ac04-fe56b359a2af",
   "metadata": {},
   "source": [
    "# Define features and target\n",
    "X = filtered_df.drop(columns=['status', 'tokens'])\n",
    "y = filtered_df['status']  # 'status' is the target variable\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d47408-23c4-40ee-bde0-5ca4b5951a19",
   "metadata": {},
   "source": [
    "#### LabelEncoder()\n",
    "\n",
    "We need to use LabelEncoder() to make sure we have correctly formatted indices for XGBoost, starting at '0'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd4030-1f6a-4472-9410-5db11b64860c",
   "metadata": {},
   "source": [
    "y_train.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a9fd9-2958-48f1-831f-c53ae929b8bd",
   "metadata": {},
   "source": [
    "# Instantiate the encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on y_train and transform it\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "\n",
    "# Transform y_test based on the encoder fitted on y_train\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "\n",
    "# Convert back to pandas Series\n",
    "y_train = pd.Series(y_train_encoded)\n",
    "y_test = pd.Series(y_test_encoded)\n",
    "\n",
    "# Check encoded labels\n",
    "print(\"Encoded y_train:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"Encoded y_test:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Mapping from encoded labels to original labels\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "\n",
    "print(\"Label Mapping:\")\n",
    "label_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675926f-f9d7-4685-8de7-84bd57391da9",
   "metadata": {},
   "source": [
    "### Vectorize - TF-IDF\n",
    "\n",
    "We need to vectorize our 'joined_tokens' again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a8e6b-fcc4-4ab6-a523-5963da84d330",
   "metadata": {},
   "source": [
    "# Step 1: Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "\n",
    "# Step 2: Transform text data\n",
    "X_train_text = vectorizer.fit_transform(X_train['joined_tokens'])\n",
    "X_test_text = vectorizer.transform(X_test['joined_tokens'])\n",
    "\n",
    "X_train_numeric = X_train.drop(columns=['joined_tokens'])\n",
    "X_test_numeric = X_test.drop(columns=['joined_tokens'])\n",
    "\n",
    "X_train_numeric = X_train[['perc_upper_words', 'neg_emoji', 'exclamation_count', 'question_count']].values\n",
    "X_test_numeric = X_test[['perc_upper_words', 'neg_emoji', 'exclamation_count', 'question_count']].values\n",
    "\n",
    "# Step 3: Combine text and numeric features (if applicable)\n",
    "# Assuming X_train_numeric and X_test_numeric are your numeric features\n",
    "X_train_combined = hstack([X_train_text, X_train_numeric])\n",
    "X_test_combined = hstack([X_test_text, X_test_numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf83cbd-79c9-4823-8d6b-aec9d00fe643",
   "metadata": {},
   "source": [
    "### Best Parameters - GridSearchCV\n",
    "\n",
    "There are so many parameters we can test for...but we'll want to limit the number we run through GridSearchCV to cut down on computational cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f13ad2-260d-425a-9411-25200c2ebd5b",
   "metadata": {},
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a7e48-78cc-481c-a4ea-506e8ee5482f",
   "metadata": {},
   "source": [
    "# Initialize XGBClassifier\n",
    "xgb_model = XGBClassifier(objective='multi:softmax', num_class=7, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_weighted',  # Use F1 score to account for class imbalance\n",
    "    cv=5,  # Number of cross-validation folds\n",
    "    verbose=0,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_combined, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1 score: {grid_search.best_score_}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f\"Test set score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ceed2-26f4-4b56-851c-00dcd2555d39",
   "metadata": {},
   "source": [
    "# Convert numerical features to sparse matrix format\n",
    "X_train_numerical = csr_matrix(X_train.drop(columns=['joined_tokens']).values)\n",
    "X_test_numerical = csr_matrix(X_test.drop(columns=['joined_tokens']).values)\n",
    "\n",
    "# Combine TF-IDF features with numerical features\n",
    "X_train_combined = hstack([X_train_text, X_train_numerical])\n",
    "X_test_combined = hstack([X_test_text, X_test_numerical])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
